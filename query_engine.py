import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from utils.chunk_utils import load_chunks
from utils.dynamic_decision import DynamicDecisionEngine # Updated import
from utils.query_parser import parse_query # New import, needed for parsed_query_details

# Load model and index
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
model = SentenceTransformer(EMBED_MODEL)

INDEX_PATH = "vector_store/faiss_index.bin"
CHUNK_PATH = "vector_store/chunks.pkl"

print("\nğŸ” Loading FAISS index...")
index = faiss.read_index(INDEX_PATH)
chunks = load_chunks(CHUNK_PATH)

assert index.ntotal == len(chunks), "Mismatch between index and chunks"

# Initialize DynamicDecisionEngine
decision_engine = DynamicDecisionEngine()

# Enhanced semantic search with similarity scores
def search_similar_chunks_with_scores(user_query, top_k=5):
    embedding = model.encode([user_query])
    distances, indices = index.search(np.array(embedding), top_k)
    
    results = []
    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
        # Convert distance to similarity score (lower distance = higher similarity)
        similarity = 1 / (1 + distance)
        results.append({
            'rank': i + 1,
            'chunk': chunks[idx],
            'similarity_score': similarity,
            'relevance': 'High' if similarity > 0.7 else 'Medium' if similarity > 0.5 else 'Low'
        })
    
    return results

# Enhanced display function for chunks
def display_chunks_nicely(chunk_results):
    print("\n" + "="*80)
    print("ğŸ“š KNOWLEDGE BASE REFERENCES")
    print("="*80)
    
    for result in chunk_results:
        print(f"\nğŸ“– Reference #{result['rank']} | Relevance: {result['relevance']} | Score: {result['similarity_score']:.3f}")
        print("-" * 60)
        
        # Show chunk content with better formatting
        chunk_text = result['chunk']
        if len(chunk_text) > 300:
            chunk_text = chunk_text[:300] + "..."
        
        # Add indentation for better readability
        formatted_chunk = '\n'.join(['    ' + line for line in chunk_text.split('\n')])
        print(f"{formatted_chunk}")
        print("-" * 60)

# Enhanced response formatting to handle new structured output
def format_final_response(response_json, chunk_results):
    print("\n" + "="*80)
    print("ğŸ¯ FINAL DECISION & REASONING")
    print("="*80)
    
    parsed_response = response_json # We now expect a parsed JSON object directly
        
    # Display main decision
    if 'decision' in parsed_response:
        print(f"\nâœ… DECISION: {parsed_response['decision']}")
    
    # Display amount
    if 'amount' in parsed_response:
        print(f"\nğŸ’° AMOUNT: {parsed_response['amount']}")
    
    # Display justification
    if 'justification' in parsed_response:
        print(f"\nğŸ§  JUSTIFICATION:")
        justification = parsed_response['justification']
        formatted_justification = '\n'.join(['    ' + line for line in justification.split('\n')])
        print(f"{formatted_justification}")
    
    # Display confidence
    if 'confidence' in parsed_response:
        print(f"\nğŸ“Š CONFIDENCE: {parsed_response['confidence']}")
    
    # Display other fields from dynamic_decision (if present)
    for key, value in parsed_response.items():
        if key not in ['decision', 'amount', 'justification', 'confidence']:
            print(f"\nğŸ“Œ {key.replace('_', ' ').upper()}:")
            if isinstance(value, dict):
                for k, v in value.items():
                    print(f"    â€¢ {k}: {v}")
            elif isinstance(value, list):
                for item in value:
                    print(f"    â€¢ {item}")
            else:
                print(f"    {value}")
            
    # Show source summary
    print(f"\nğŸ“‹ SOURCES SUMMARY:")
    high_rel = sum(1 for r in chunk_results if r['relevance'] == 'High')
    med_rel = sum(1 for r in chunk_results if r['relevance'] == 'Medium')
    low_rel = sum(1 for r in chunk_results if r['relevance'] == 'Low')
    
    print(f"    ğŸ”´ High Relevance: {high_rel} sources")
    print(f"    ğŸŸ¡ Medium Relevance: {med_rel} sources")
    print(f"    ğŸ”µ Low Relevance: {low_rel} sources")
    
    if len(chunk_results) > 0:
        avg_score = sum(r['similarity_score'] for r in chunk_results) / len(chunk_results)
        print(f"    ğŸ“ˆ Average Similarity Score: {avg_score:.3f}")
    else:
        print(f"    ğŸ“ˆ Average Similarity Score: N/A")

# Main processing function
def process_user_query(user_input):
    print(f"\nğŸ’¬ User Query: {user_input}")
    print("="*50)
    
    # 1. Parse details
    parsed_query_details = parse_query(user_input) # Get parsed details
    print(f"ğŸ” Parsed Query Details: {parsed_query_details}")
    
    if all(value is None for value in parsed_query_details.values()):
        print("â„¹ï¸ No structured information found â€” proceeding with raw query.\n")

    # 2. Enhanced semantic search with scores
    chunk_results = search_similar_chunks_with_scores(user_input, top_k=5)
    
    # 3. Display chunks nicely
    display_chunks_nicely(chunk_results)
    
    # 4. Get LLM decision using the DynamicDecisionEngine
    matched_chunks = [result['chunk'] for result in chunk_results]
    
    # Pass parsed_query_details to the decision engine
    json_response_str = decision_engine.make_decision_from_context(user_input, parsed_query_details, matched_chunks)
    
    try:
        response_json = json.loads(json_response_str)
    except json.JSONDecodeError:
        print("âŒ Failed to parse LLM response as JSON. Raw response:")
        print(json_response_str)
        return None

    # 5. Format and display final response
    format_final_response(response_json, chunk_results)
    
    return response_json

# Enhanced main loop with better UI
def main():
    print("\n" + "="*80)
    print("ğŸ” DOCUMENT QUERY ASSISTANT")
    print("="*80)
    print("Ask questions about your documents. Type 'exit' or 'quit' to stop.")
    print("Examples:")
    print("  â€¢ What are the eligibility criteria for insurance?")
    print("  â€¢ What is the age limit for policy applications?")
    print("  â€¢ What documents are required for claims?")
    print("  â€¢ If I am a senior citizen, what benefits are covered?")
    print("  â€¢ 46-year-old male, knee surgery in Pune, 3-month-old insurance policy")
    print("="*80)
    
    query_count = 0
    
    while True:
        query_count += 1
        print(f"\nğŸ¯ Query #{query_count}")
        user_query = input("ğŸ“ Enter your query: ").strip()
        
        if user_query.lower() in ("exit", "quit", "q"):
            print("\nğŸ‘‹ Thank you for using the Document Query Assistant!")
            break
        
        if not user_query:
            print("â— Please enter a valid query.")
            continue
        
        try:
            response = process_user_query(user_query)
            
            if not response:
                continue
            
            # Ask if user wants to continue
            print("\n" + "="*80)
            continue_choice = input("â“ Would you like to ask another question? (y/n): ").strip().lower()
            if continue_choice in ('n', 'no'):
                print("\nğŸ‘‹ Thank you for using the Document Query Assistant!")
                break
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Session interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ An error occurred: {e}")
            print("Please try again with a different query.")

if __name__ == "__main__":
    main()